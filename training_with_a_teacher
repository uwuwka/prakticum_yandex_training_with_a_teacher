# # Отток клиентов

# Из «Бета-Банка» стали уходить клиенты. Каждый месяц. Немного, но заметно. Банковские маркетологи посчитали: сохранять текущих клиентов дешевле, чем привлекать новых.
# 
# Нужно спрогнозировать, уйдёт клиент из банка в ближайшее время или нет. Вам предоставлены исторические данные о поведении клиентов и расторжении договоров с банком. 
# 
# Постройте модель с предельно большим значением *F1*-меры. Чтобы сдать проект успешно, нужно довести метрику до 0.59. Проверьте *F1*-меру на тестовой выборке самостоятельно.
# 
# Дополнительно измеряйте *AUC-ROC*, сравнивайте её значение с *F1*-мерой.
# 
# Источник данных: [https://www.kaggle.com/barelydedicated/bank-customer-churn-modeling](https://www.kaggle.com/barelydedicated/bank-customer-churn-modeling)

# ## Подготовка данных

from tqdm import tqdm
import pandas as pd
import matplotlib.pyplot as plt

from sklearn.preprocessing import MinMaxScaler
from sklearn.utils import shuffle
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, roc_auc_score, roc_curve, confusion_matrix
from sklearn.preprocessing import OneHotEncoder

from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.tree import DecisionTreeClassifier
from sklearn.dummy import DummyClassifier


import warnings
warnings.filterwarnings("ignore")


# сохраняем данные в переменную и смотрим общую информацию
df = pd.read_csv('/datasets/Churn.csv')
df.head()

df.info()

df.duplicated().sum()


df['Tenure'] = df['Tenure'].fillna(0)



# ## Исследование задачи

# Удалим столбцы-идентификаторы, не представляющие ценностия для алгоритма.
# создадим дополнительный датафрем, в котором будут хранится данные для обучения модели


drop_columns = ['RowNumber','CustomerId', 'Surname']
df_ml = df.drop(drop_columns, axis=1)

df_ml.shape


# Данные подготовим методом OHE, что позволит нам использовать разные модели и не словить дамми ловушку


df_ml = pd.get_dummies(df_ml, drop_first=True)

df_ml.shape


# Разделим на признаки и целевой признак

features = df_ml.drop('Exited', axis=1)
target = df_ml['Exited']



features_train, features_test, target_train, target_test = train_test_split(features,
                                                    target,
                                                    train_size=0.6,
                                                    random_state=12345)
features_valid, features_test, target_valid, target_test = train_test_split(features_test,
                                                    target_test,
                                                    train_size=0.5,
                                                    random_state=12345)


# посмотрим исходные валидационные данные, чтобы сравнить их с от маштабированными в дальнейшем

features_valid.head()

features_valid.shape


# посмотрим исходные тренировочные данные, чтобы сравнить их с от маштабированными в дальнейшем

features_train.head()

features_train.shape


# посмотрим исходные тестовые данные, чтобы сравнить их с от маштабированными в дальнейшем

features_train.head()



features_test.shape



enc = OneHotEncoder(handle_unknown = 'ignore')
enc.fit(features_train)



enc.transform(features_train)
enc.transform(features_test)
enc.transform(features_valid)
enc.transform(features_test)


# Для масштабирования методом scaler зафиксируем численные признаки

numeric = ['CreditScore', 'Age', 'Tenure', 'Balance', 'NumOfProducts', 'EstimatedSalary']


scaler = StandardScaler()
scaler.fit(features_train[numeric])


# Масштабируем численные признаки обучающей выборки


features_train[numeric] = scaler.transform(features_train[numeric])
features_train.head()


# Масштабируем численные признаки валидационной выборки 



features_valid[numeric] = scaler.transform(features_valid[numeric])
features_valid.head()


# Масштабируем численные признаки тестовой выборки 


features_test[numeric] = scaler.transform(features_test[numeric])
features_test.head()


# строим диаграмму, которая наглядно покажет дисбаланс данных на тренировочной выборке


df_ml['Exited'].plot(kind ='hist', bins=2, figsize=(5,5))


# строим диаграмму на тестовой выборке, чтобы убедиться в том, что дисбаланс есть и это не простая ошибка в тренировочной выборке.

# Как мы выяснили в нашей выборке отрицательны ответов ≈80% , положитительных ≈ 20%. С уверенностью можем сказать что дисбаланс присутствует и он существенный.

# напишем функцию для изучия полноты, точности и F1-меры

def rec_prec_f1(target_valid, prediction):
    print("Полнота" , recall_score(target_valid, prediction))
    print("Точность", precision_score(target_valid, prediction))
    print("F1-мера", f1_score(target_valid, prediction))


# обучаем модели

best_RF = None
best_accuracy_RF = 0
best_f1_RF=0
best_est_RF = 0
best_depth_RF = 0
for est in tqdm(range(10,200,10)):
    for depth in range(2,25):
        RF = RandomForestClassifier(random_state = 12345,n_estimators = est, max_depth = depth)
        RF.fit(features_train,target_train)
        prediction_valid_RF = RF.predict(features_valid)
        accuracy_RF = accuracy_score(prediction_valid_RF, target_valid)
        f1_RF = f1_score(prediction_valid_RF, target_valid)
        if best_f1_RF < f1_RF:
            best_f1_RF = f1_RF
            best_RF = RF
            best_depth_RF = depth
            best_est_RF = est
            best_accuracy_RF = accuracy_RF

RF_probabilities_one_valid = best_RF.predict_proba(features_valid)[:, 1]

print(rec_prec_f1(target_valid,prediction_valid_RF))
print('AUC-ROC',roc_auc_score(target_valid, RF_probabilities_one_valid))
print()
print('лучшая модель с параметрами:', 'глубина-',best_depth_RF,'','количество ветвей-',best_est_RF)


best_DT = None
best_accuracy_DT = 0
best_f1_DT = 0
best_depth_DT = 0
for depth in tqdm(range(2,52)):
    DT = DecisionTreeClassifier(random_state = 12345, max_depth = depth)
    DT.fit(features_train,target_train)
    prediction_valid_DT = DT.predict(features_valid)
    accuracy_DT = accuracy_score(prediction_valid_DT, target_valid)
    f1_DT = f1_score(prediction_valid_DT, target_valid)
    if best_f1_DT < f1_DT:
        best_f1_DT = f1_DT
        best_DT = DT
        best_accuracy_DT = accuracy_DT
        best_depth_DT = depth

DT_probabilities_one_valid = best_DT.predict_proba(features_valid)[:, 1]

print(rec_prec_f1(target_valid,prediction_valid_DT))
print('AUC-ROC',roc_auc_score(target_valid, DT_probabilities_one_valid))
print()
print('лучшая модель с параметрами:', 'глубина-',best_depth_DT,)


best_LR = None
best_f1_LR=0
best_accuracy_LR = 0
for iter_ in tqdm([100, 200, 500, 1000, 5000]):
    LR = LogisticRegression(random_state = 12345,max_iter=iter_,tol=1e-5,solver = 'lbfgs' )
    LR.fit(features_train,target_train)
    prediction_valid_LR = LR.predict(features_valid)
    accuracy_LR = accuracy_score(prediction_valid_LR, target_valid)
    f1_LR = f1_score(prediction_valid_LR, target_valid)
    if best_f1_LR < f1_LR:
        best_LR = LR
        best_accuracy_LR = accuracy_LR

LR_probabilities_one_valid = best_LR.predict_proba(features_valid)[:, 1]            

print(rec_prec_f1(target_valid,prediction_valid_LR))
print('AUC-ROC',roc_auc_score(target_valid, LR_probabilities_one_valid))

# Вывод: Видим низкое значение F1, следовательно низкое качество моделей.

# Функция для отображения соотношения ответов моделей (сколько 0, сколько 1)


def all_models_share(features_train, target_train, features_valid, target_valid):
    model_DT = DecisionTreeClassifier(random_state = 12345, max_depth = 9)
    model_DT.fit(features_train, target_train)
    DT_share = pd.Series(model_DT.predict(features_valid)).value_counts(normalize = 1)



    model_RF = RandomForestClassifier(random_state = 12345,n_estimators = 40, max_depth = 16)
    model_RF.fit(features_train, target_train)
    RF_share = pd.Series(model_RF.predict(features_valid)).value_counts(normalize = 1)

    model_LR = LogisticRegression(random_state = 12345,max_iter= 1000,tol=1e-5,solver = 'lbfgs')
    model_LR.fit(features_train, target_train)
    LR_share = pd.Series(model_LR.predict(features_valid)).value_counts(normalize = 1)



    print("Доли ответов:")
    print()
    print("Дерево решений", DT_share)
    print()
    print("Случайный лес ", RF_share)
    print()
    print("Логистческая регрессия", LR_share)


# Применим функцию отображения соотношения ответов моделей



all_models_share(features_train, target_train, features_valid, target_valid)


# вывод: Логистичесая регрессия показала болюшую долю положительных ответов, далее случайный лес и в конце дерево решений
# С учетом дисбаланса неудивительно, что модели с большой вероятностью будут выдавать ответ 0, построим матрицы ошибок для моделей


# Матрица ошибок для дерево решений

confusion_matrix(target_valid, prediction_valid_DT)


# Матрица ошибок для случайный лес

confusion_matrix(target_valid, prediction_valid_RF)


# Вывод: Матрица показала, что дерево решений склонно выдавать позитивные предсказания, очень высокое количество ложных позитивных предсказания (FP).

# поработаем с логистической регрессией отдельно

confusion_matrix(target_valid, prediction_valid_LR)


def plot_roc_curve(fper, tper):
    plt.plot(fper, tper, color='red', label='ROC')
    plt.plot([0, 1], [0, 1], color='green', linestyle='--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.0])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('ROC-кривая')
    plt.legend()
    plt.show()


fper, tper, thresholds = roc_curve(target_valid, LR_probabilities_one_valid)
plot_roc_curve(fper, tper)


# Попробуем обучать логистическую регресию сбалансировав классы

model_LR = LogisticRegression(random_state = 12345,max_iter= 1000,tol=1e-5,solver = 'lbfgs')
model_LR.fit(features_train, target_train)
LR_probabilities_one_valid_class_weight = model_LR.predict_proba(features_valid)[:, 1]
print("Score", model_LR.score(features_valid, target_valid))
print("AUC-ROC", roc_auc_score(target_valid, LR_probabilities_one_valid_class_weight))

fper, tper, thresholds = roc_curve(target_valid, LR_probabilities_one_valid_class_weight) 
plot_roc_curve(fper, tper)


# Вывод: Отстутсвие улучшений - тоже результат. Высокая точность модели объясняется высокой долей негативных ответов в валидационной выборке.

# ## Борьба с дисбалансом

# Как мы выяснили ранее в нашей выборке отрицательны ответов ≈80% , положитительных ≈ 20%. Нам необходмо увеличить количество положительных ответов в 4 раза для достижения баланса. Либо же уменьшить кол-во отрицтаельных ответов.

# Создадим функцию для увеличения представленной класса в выборке 

def upsample(features, target, repeat, upsampled_сlass):
    """Функция принимаем значение признаков (features[]), целевого признака (target[]), repeat(int / float),
    класс который будет увеличен (upsampled_сlass (0 or 1))"""
    features_zeros = features[target == 0]
    features_ones = features[target == 1]
    target_zeros = target[target == 0]
    target_ones = target[target == 1]

    if upsampled_сlass == 0:
        features_upsampled = pd.concat([features_zeros]* repeat + [features_ones] )
        target_upsampled = pd.concat([target_zeros]* repeat + [target_ones] )
        features_upsampled, target_upsampled = shuffle(
        features_upsampled, target_upsampled, random_state=12345)

    elif upsampled_сlass == 1:
        features_upsampled = pd.concat([features_zeros] + [features_ones] * repeat)
        target_upsampled = pd.concat([target_zeros] + [target_ones] * repeat)
        features_upsampled, target_upsampled = shuffle(
        features_upsampled, target_upsampled, random_state=12345)
    else:
        features_upsampled = 0
        target_upsampled = 0  



    return features_upsampled, target_upsampled


# Создадим функцию для уменьшения представленной класса в выборке 

def downsample(features, target, fraction):
    features_zeros = features[target == 0]
    features_ones = features[target == 1]
    target_zeros = target[target == 0]
    target_ones = target[target == 1]

    features_downsampled = pd.concat(
        [features_zeros.sample(frac=fraction, random_state=12345)] + [features_ones])
    target_downsampled = pd.concat(
        [target_zeros.sample(frac=fraction, random_state=12345)] + [target_ones])

    features_downsampled, target_downsampled = shuffle(
        features_downsampled, target_downsampled, random_state=12345)

    return features_downsampled, target_downsampled


# Применим функцию upsample.
# увеличим количество положительных ответов в 4 раза.
# Протестируем функцию.


features_train_upsampled, target_train_upsampled = upsample(features_train, target_train, 4, 1)
print(target_train_upsampled.value_counts(normalize = 1))
print(target_train_upsampled.shape)


# Применим функцию upsample 
# увеличим количество положительных ответов в 4 раза

features_train_upsampled, target_train_upsampled = upsample(features_train, target_train, 4, 1)
print(target_train_upsampled.value_counts(normalize = 1))
print(target_train_upsampled.shape)


# Применим функцию downsample.
# Уменьшим кол-в пооложительных ответов в 4 раза.
# Протестируем функцию.

features_downsampled_train, target_downsampled_train = downsample(features_train, target_train, 0.25)
print(target_downsampled_train.value_counts(normalize = 1))
print(target_downsampled_train.shape)


target_train_upsampled.plot(kind ='hist', bins=3, figsize=(5,5))

target_downsampled_train.plot(kind ='hist', bins=3, figsize=(5,5))


# вывод:видим,что функции выдали одинаковый результат. Есть смысл проверить обе

# проверим функцию 'upsampled'

# Решающее дерево

best_DT_upsampled = None
best_accuracy_DT_upsampled = 0
best_f1_DT_upsampled=0
best_depth_DT_upsampled = 0
for depth in tqdm(range(2,52)):
    DT_upsampled = DecisionTreeClassifier(random_state = 12345, max_depth = depth)
    DT_upsampled.fit(features_train_upsampled, target_train_upsampled)
    prediction_valid_DT_upsampled = DT_upsampled.predict(features_valid)
    accuracy_DT_upsampled = accuracy_score(prediction_valid_DT_upsampled, target_valid)
    f1_DT_upsampled = f1_score(prediction_valid_DT_upsampled, target_valid)
    if best_f1_DT_upsampled < f1_DT_upsampled:
        best_f1_DT_upsampled = f1_DT_upsampled
        best_DT_upsampled = DT_upsampled
        best_accuracy_DT_upsampled = accuracy_DT_upsampled
        best_depth_DT_upsampled = depth

DT_upsampled_probabilities_one_valid = best_DT_upsampled.predict_proba(features_valid)[:, 1]

print(rec_prec_f1(target_valid,prediction_valid_DT_upsampled))
print('AUC-ROC',roc_auc_score(target_valid, DT_upsampled_probabilities_one_valid))
print()
print('лучшая модель с параметрами:', 'глубина-',best_depth_DT_upsampled)


# Случайный лес


best_RF_upsampled = None
best_accuracy_RF_upsampled = 0
best_f1_RF_upsampled = 0
best_est_RF_upsampled = 0
best_depth_RF_upsampled = 0
for est in tqdm(range(1013,20000,10100)):
    for depth in range(2,500):
        RF_upsampled = RandomForestClassifier(random_state = 12345,n_estimators = est, max_depth = depth)
        RF_upsampled.fit(features_train_upsampled, target_train_upsampled)
        prediction_valid_RF_upsampled = RF_upsampled.predict(features_valid)
        accuracy_RF_upsampled = accuracy_score(prediction_valid_RF_upsampled, target_valid)
        f1_RF_upsampled = f1_score(prediction_valid_RF_upsampled, target_valid)
        if best_f1_RF_upsampled < f1_RF_upsampled:
            best_f1_RF_upsampled = f1_RF_upsampled
            best_RF_upsampled = RF_upsampled
            best_depth_RF_upsampled = depth
            best_est_RF_upsampled = est
            best_accuracy_RF_upsampled = accuracy_RF_upsampled


RF_upsampled_probabilities_one_valid = best_RF_upsampled.predict_proba(features_valid)[:, 1]

print(rec_prec_f1(target_valid,prediction_valid_RF_upsampled))
print('AUC-ROC',roc_auc_score(target_valid, RF_upsampled_probabilities_one_valid))
print()
print('лучшая модель с параметрами:', 'глубина-',best_depth_RF_upsampled,'','количество ветвей-',best_est_RF_upsampled)


# Логистическая регрессия

best_LR_upsampled = None
best_f1_LR_upsampled=0
best_accuracy_LR_upsampled = 0
for est in tqdm(range(2,52)):
    LR_upsampled = LogisticRegression(random_state = 12345,max_iter= 1000,tol=1e-5,solver = 'liblinear' )
    LR_upsampled.fit(features_train_upsampled, target_train_upsampled)
    LR_prediction_upsampled = LR_upsampled.predict(features_valid)
    accuracy_LR_upsampled = accuracy_score(LR_prediction_upsampled, target_valid)
    f1_LR_upsampled = f1_score(LR_prediction_upsampled, target_valid)
    if best_f1_LR_upsampled < f1_LR_upsampled:
        best_f1_LR_upsampled = f1_LR_upsampled
        best_LR_upsampled = LR_upsampled
        best_accuracy_LR_upsampled = accuracy_LR_upsampled

LR_upsampled_probabilities_one_valid = best_LR_upsampled.predict_proba(features_valid)[:, 1]            

print(rec_prec_f1(target_valid, LR_prediction_upsampled))
print('AUC-ROC',roc_auc_score(target_valid, LR_upsampled_probabilities_one_valid))

# проверим метод 'downsample'

# Решающее дерево



best_DT_downsampled = None
best_accuracy_DT_downsampled = 0
best_f1_DT_downsampled=0
best_depth_DT_downsampled = 0
for depth in tqdm(range(2,52)):
    DT_downsampled = DecisionTreeClassifier(random_state = 12345, max_depth = depth)
    DT_downsampled.fit(features_downsampled_train, target_downsampled_train)
    prediction_valid_DT_downsampled = DT_downsampled.predict(features_valid)
    accuracy_DT_downsampled = accuracy_score(prediction_valid_DT_downsampled, target_valid)
    f1_DT_downsampled= f1_score(prediction_valid_DT_downsampled, target_valid)
    if best_f1_DT_downsampled < f1_DT_downsampled:
        best_f1_DT_downsampled = f1_DT_downsampled
        best_DT_downsampled = DT_downsampled
        best_accuracy_DT_downsampled = accuracy_DT_downsampled
        best_depth_DT_downsampled = depth

DT_downsampled_probabilities_one_valid = best_DT_downsampled.predict_proba(features_valid)[:, 1]

print(rec_prec_f1(target_valid,prediction_valid_DT_downsampled))
print('AUC-ROC',roc_auc_score(target_valid, DT_downsampled_probabilities_one_valid))
print()
print('лучшая модель с параметрами:', 'глубина-',best_depth_DT_downsampled)


# Случайный лес


best_RF_downsampled = None
best_accuracy_RF_downsampled = 0
best_f1_RF_downsampled=0
best_est_RF_downsampled = 0
best_depth_RF_downsampled = 0
for est in tqdm(range(10,200,10)):
    for depth in range(2,25):
        RF_downsampled = RandomForestClassifier(random_state = 12345,n_estimators = est, max_depth = depth)
        RF_downsampled.fit(features_downsampled_train, target_downsampled_train)
        prediction_valid_RF_downsampled = RF_downsampled.predict(features_valid)
        accuracy_RF_downsampled = accuracy_score(prediction_valid_RF_downsampled, target_valid)
        f1_RF_downsampled = f1_score(prediction_valid_RF_downsampled, target_valid)
        if best_f1_RF_downsampled < f1_RF_downsampled:
            best_f1_RF_downsampled = f1_RF_downsampled
            best_RF_downsampled = RF_downsampled
            best_depth_RF_downsampled = depth
            best_est_RF_downsampled = est
            best_accuracy_RF_downsampled = accuracy_RF_downsampled


RF_downsampled_probabilities_one_valid = best_RF_downsampled.predict_proba(features_valid)[:, 1]

print(rec_prec_f1(target_valid,prediction_valid_RF_downsampled))
print('AUC-ROC',roc_auc_score(target_valid, RF_downsampled_probabilities_one_valid))
print()
print('лучшая модель с параметрами:', 'глубина-',best_depth_RF_downsampled,'','количество ветвей-',best_est_RF_downsampled)


# Логистическая регрессия



best_LR_downsampled = None
best_f1_LR_downsampled=0
best_accuracy_LR_downsampled = 0
for est in tqdm(range(2,52)):
    LR_downsampled = LogisticRegression(random_state = 12345,max_iter= 1000,tol=1e-5,solver = 'liblinear' )
    LR_downsampled.fit(features_downsampled_train, target_downsampled_train)
    LR_prediction_downsampled = LR_downsampled.predict(features_valid)
    accuracy_LR_downsampled = accuracy_score(LR_prediction_downsampled, target_valid)
    f1_LR_downsampled = f1_score(LR_prediction_downsampled, target_valid)
    if best_f1_LR_downsampled < f1_LR_downsampled:
        best_f1_LR_downsampled = f1_LR_downsampled
        best_LR_downsampled = LR_downsampled
        best_accuracy_LR_downsampled = accuracy_LR_downsampled

LR_downsampled_probabilities_one_valid = best_LR_downsampled.predict_proba(features_valid)[:, 1]            

print(rec_prec_f1(target_valid, LR_prediction_downsampled))
print('AUC-ROC',roc_auc_score(target_valid, LR_downsampled_probabilities_one_valid))


# проверим Точность моделей на выборке с дисбалансом



print(best_accuracy_RF)
print(best_accuracy_DT)
print(best_accuracy_LR)


# проверим Точность моделей на сбалансированной увеличенной выборке



print(best_accuracy_RF_upsampled)
print(best_accuracy_DT_upsampled)
print(best_accuracy_LR_upsampled)


# проверим Точность моделей на сбалансированной уменьшенной выборке


print(best_accuracy_RF_downsampled)
print(best_accuracy_DT_downsampled)
print(best_accuracy_LR_downsampled)


# ## Тестирование модели

# Обучим финальную модель с обьединенными данными



features_full_train = pd.concat([features_train, features_valid])
target_full_train = pd.concat([target_train, target_valid])



features_upsampled, target_upsampled = upsample(features_full_train, target_full_train, 4, 1)


RF_final_full_data = RandomForestClassifier(random_state = 12345,n_estimators = 150, max_depth = 13)
RF_final_full_data.fit(features_upsampled, target_upsampled)

prediction_valid_RF_final_full_data = RF_final_full_data.predict(features_test)

f1_RF_final_full_data= f1_score(prediction_valid_RF_final_full_data, target_test)

RF_full_data_probabilities_one_valid = RF_final_full_data.predict_proba(features_test)[:, 1]

print(rec_prec_f1(target_test,prediction_valid_RF_final_full_data))
print('AUC-ROC',roc_auc_score(target_test, RF_full_data_probabilities_one_valid))


# Обучим финальную модель без обьединенных данных

RF_final = RandomForestClassifier( max_depth= 12,  n_estimators = 20, random_state=12345)
RF_final.fit(features_train_upsampled, target_train_upsampled)

RF_final_prediction =RF_final.predict(features_test)

RF_final_valid = RF_final.predict_proba(features_test)[:, 1]

auc_roc_RF = roc_auc_score(target_test, RF_final_valid)

print(rec_prec_f1(target_test, RF_final_prediction))
print('AUC-ROC',roc_auc_score(target_test, RF_final_valid))


# вывод: модель обученная на обьедененных данных оказалась менее точной (даже после маштобирования данных) по F-1 мере, а вот AUC-ROC у нее оказался выше,но мы в первую очередь смотрим на F-1 меру по этому мы будем исполоьзовать модель обученную на не обьеденненых данных.

# Создаем константную модель

target_predict_constant = pd.Series([0]*len(target_test))
target_predict_constant.value_counts()

# Сравним показатель точности (accuracy_score) константной модели и финальной

print('accuracy_score константой модели:', accuracy_score(target_valid, target_predict_constant))
print('accuracy_score финальной модели:', accuracy_score(target_test, RF_final_prediction))

# Дополнительно сравним AUC-ROC — единственный параметр подающийся сравнению, потому что константная подель содержит только негативные ответы

print('AUC-ROC константой модели:', roc_auc_score(target_valid, target_predict_constant))
print('AUC-ROC финальной модели:', roc_auc_score(target_test, RF_final_valid))


# вывод:Финальная модель показывает результаты лучше, чем константная модель — модель можно считать адекватной.


# ## Вывод

# Итоговый вывод:В первоначальные данных наблюдался значительный дисбаланс (80% ответов целевого признака были негативными и только 20% позитивными), из-за чего обученная на этих данных модель не проходила проверку на адекватность. Все модели не первоначальных данных характеризовались высокой степенью ошибок и низким качеством взвешенной величины (F1) — модели показывали низкие результаты точности и полноты.
# 
# Мы устранили дисбаланс классов в обучающей выборки методом upsampling — увеличили количество значений позитивного класса в 4 раза. Так мы достигли баланса классов обучаюющей выборки: 0 - 0.501043 1 - 0.498957.
# 
# также использовали метод downsampling- уменьшили количество значений позитивного класса в 4 раза.Получили такой же результат как и у метода upsampling: 0 - 0.501043 1 - 0.498957.НО этот метод сильно ухудшает качество моделей из-за недостатка данных для обучения.По этой причине в дальнейшем был использован метод upsampling ктоторый ухудшает качество модели не столь критично.
# 
# Разобрали несколько вариантов борьбы с дисбалансом upsampling и downsampling
# 
# На новых данных все модели показали результат выше, чем на несбалансированной выборке. Лучшие показатели были у модели случайного леса:
# 
# Полнота 0.5515151515151515
# Точность 0.674074074074074
# F1-мера 0.6066666666666666
# AUC-ROC 0.8339155332856119
# лучшая модель с параметрами: глубина- 12  количество ветвей- 20 и рандомное состояне- 12345
# 
# была расмотренная финальная модель с обьеденненой валидационной и тренеровочной выборками, она показала результат хуже чем модель обученная на необьеденненой маштабированной быборке, по этой причине была отвергнута как не подходящая по значению F-1 меры.
# Финальная модель прошла проверку на адекватность в сравнении с контантной моделью: accuracy_score константой модели: 0.85
# accuracy_score константой модели: 0.791
# accuracy_score финальной модели: 0.837
# AUC-ROC константой модели: 0.5
# AUC-ROC финальной модели: 0.8495744830760144
# модель с параметрами: глубина- 12  количество ветвей- 20 и рандомное состояне- 12345

# ## Чек-лист готовности проекта

# Поставьте 'x' в выполненных пунктах. Далее нажмите Shift+Enter.

# - [x]  Jupyter Notebook открыт
# - [x]  Весь код выполняется без ошибок
# - [x]  Ячейки с кодом расположены в порядке исполнения
# - [x]  Выполнен шаг 1: данные подготовлены
# - [x]  Выполнен шаг 2: задача исследована
#     - [x]  Исследован баланс классов
#     - [x]  Изучены модели без учёта дисбаланса
#     - [x]  Написаны выводы по результатам исследования
# - [x]  Выполнен шаг 3: учтён дисбаланс
#     - [x]  Применено несколько способов борьбы с дисбалансом
#     - [x]  Написаны выводы по результатам исследования
# - [x]  Выполнен шаг 4: проведено тестирование
# - [x]  Удалось достичь *F1*-меры не менее 0.59
# - [x]  Исследована метрика *AUC-ROC*
